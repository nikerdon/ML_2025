{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06ccdfe-ea70-4f14-9cb1-76ae726fba84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392870e4-59d0-418b-b830-c8b718e629e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Python\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd93ee-7eb5-4747-ae5a-65da507e4d8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load Datasets and Models\n",
    "(Be careful of the sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd55df-e7e0-4041-a1ec-f59ce2aea555",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('X_train_phy.pkl') # train with features close to those \n",
    "#X_train = pd.read_pickle('X_train_pca.pkl') # train with principle components of dataset\n",
    "y_train = pd.read_pickle('y_train.pkl')\n",
    "#X_test = pd.read_pickle('X_test_phy.pkl')\n",
    "#X_test = pd.read_pickle('X_test_pca.pkl')\n",
    "#y_test = pd.read_pickle('y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447db44-0092-4626-88be-1208f9056ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_file(filename = 'model.pkl'):\n",
    "    with open(filename, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    #print(f\"model загрука в файл: {filename}\")\n",
    "    print(\"Load model: \", filename)\n",
    "    print(model) # this is how to figure out what object is stored here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af0016-3048-4087-944b-6be9cc369077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load any number of models into a list\n",
    "model = []\n",
    "#model.append(read_model_file('tree_model.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e45aaa-7c4b-4fcd-8c9f-dbfdbb156c92",
   "metadata": {},
   "source": [
    "# Find models\n",
    "\n",
    "score:\n",
    "f1 seems like the best option. It is a standard measure which works on binary classifiers with uneven class sizes. Trying something better would require much more time and energy.\n",
    "\n",
    "classifiers:\n",
    " -- I currently have trees on the PCA input, and am running such random forest. I believe that we should try the trees, but without PCA. We need to decide on minimum sample size and max depth. With such a large dataset, the minimum sample size should be large.\n",
    "\n",
    " -- kNN seems worthless in our case. It is not very useful for large datasets, so we need to have a reason to believe it is useful, and a means to shrink the dataset (like averaging nearby points).\n",
    "\n",
    " -- Naive Bayes would be good to run on the output of PCA. In this case, PCA removes the correlation, making it very useful, while the physical relevance of the features is not actually helpful.\n",
    "\n",
    " -- SVM seems to have a problem with the large sample size. I would like to run it, if possible, with a non-linear kernel, but I need to figure out the runtime issue. I would consider this an \"if we have time\" option -- analyze everything without it.\n",
    "https://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use\n",
    "\n",
    " -- We could try some other ensemble model version, if someone is creative enough to think of one. Splitting the data and averaging the results is easy enough -- just don't use the same sample too many times (1 time is prefered). These ensemble versions could enable us to take advantage of the large number of samples, rather than causing them to slow down the evaluation. \n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e851273-3363-417f-b24b-3eccfdbf2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "# parameters to provide\n",
    "tree_param_grid = {'max_depth': [3, 5, 10], 'min_samples_split': [5, 10, 15]}\n",
    "scoring         = 'f1'\n",
    "model_filename  = f\"tree_{scoring}_{tree_param_grid['max_depth'][-1]}_{tree_param_grid['min_sample_size'][0]}.pkl\"\n",
    "n_jobs          = 1      # put number of processors here\n",
    "cv              = 5\n",
    "\n",
    "# run model, with timing\n",
    "t_start = time.time()\n",
    "test_model = GridSearchCV(DecisionTreeClassifier(), tree_param_grid, scoring = scoring, n_jobs = n_jobs, cv=cv)\n",
    "test_model.fit(X_train, y_train)\n",
    "t_finish = time.time()\n",
    "dt = t_finish - t_start\n",
    "dt2 = dt / 60.\n",
    "print(\"Decision Tree Best Params:\", test_model.best_params_)\n",
    "print(\"Time: \",  dt, \" sec or \", dt2, \"min\")\n",
    "\n",
    "# Сохранение модели в файл\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(test_model, file)\n",
    "print(f\"Модель сохранена в файл: {model_filename}\")\n",
    "\n",
    "#save model in the model array\n",
    "#model.append(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948495b-f960-44a0-b148-0c28dde84e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# parameters to provide\n",
    "rf_param_grid   = {'n_estimators': [10, 50, 100, 200], 'max_depth': [5, 10, 15], 'min_samples_split': [3, 5, 10]}\n",
    "scoring         = 'f1'\n",
    "model_filename  = f\"forest_{scoring}_{rf_param_grid['max_depth'][-1]}_{rf_param_grid['min_sample_size'][0]}.pkl\"\n",
    "n_jobs          = 8      # put number of processors here\n",
    "cv              = 5\n",
    "\n",
    "# run model, with timing\n",
    "t_start = time.time()\n",
    "test_model = GridSearchCV(RandomForestClassifier(), rf_param_grid, scoring = scoring, n_jobs = n_jobs, cv=cv)\n",
    "test_model.fit(X_train, y_train)\n",
    "t_finish = time.time()\n",
    "dt = t_finish - t_start\n",
    "dt2 = dt / 60.\n",
    "print(\"Random Forest Best Params:\", test_model.best_params_)\n",
    "print(\"Time: \",  dt, \" sec or \", dt2, \"min\")\n",
    "\n",
    "# Сохранение модели в файл\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(test_model, file)\n",
    "print(f\"Модель сохранена в файл: {model_filename}\")\n",
    "\n",
    "#save model in the model array\n",
    "#model.append(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9892e6-ca80-438c-a09a-6e53a572436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine (SVM)\n",
    "# Don't run -- requires too much time\n",
    "\n",
    "# parameters to provide\n",
    "svm_param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "scoring         = 'f1'\n",
    "model_filename  = f\"svm_model_{scoring}}.pkl\"\n",
    "n_jobs          = 1      # put number of processors here\n",
    "cv              = 5\n",
    "\n",
    "# run model, with timing\n",
    "t_start = time.time()\n",
    "test_model = GridSearchCV(SVC(), svm_param_grid, scoring = scoring, n_jobs = n_jobs, cv=cv)\n",
    "test_model.fit(X_train, y_train)\n",
    "t_finish = time.time()\n",
    "dt = t_finish - t_start\n",
    "dt2 = dt / 60.\n",
    "print(\"SVC Best Params:\", test_model.best_params_)\n",
    "print(\"Time: \",  dt, \" sec or \", dt2, \"min\")\n",
    "\n",
    "# Сохранение модели в файл\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(test_model, file)\n",
    "print(f\"Модель сохранена в файл: {model_filename}\")\n",
    "\n",
    "#save model in the model array\n",
    "#model.append(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2868664-c071-4613-b841-8871e508a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "# parameters to provide\n",
    "model_filename  = f\"bayes_model.pkl\"\n",
    "\n",
    "# run model, with timing\n",
    "t_start = time.time()\n",
    "test_model = GaussianNB()\n",
    "test_model.fit(X_train, y_train)\n",
    "t_finish = time.time()\n",
    "dt = t_finish - t_start\n",
    "dt2 = dt / 60.\n",
    "print(\"Time: \",  dt, \" sec or \", dt2, \"min\")\n",
    "\n",
    "# Сохранение модели в файл\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(test_model, file)\n",
    "print(f\"Модель сохранена в файл: {model_filename}\")\n",
    "\n",
    "#save model in the model array\n",
    "#model.append(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d7fbc-de3b-4fc0-84a8-5281289a1725",
   "metadata": {},
   "source": [
    "# Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4347d95b-d1d4-483a-a498-b8f14c31ff6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
